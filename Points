config files  are at location - /etc/hadoop/conf

core-site.xml - fs.defaultFs gives name node URI

<property>
      <name>fs.defaultFS</name>
      <value>hdfs://nn01.itversity.com:8020</value>
      <final>true</final>
    </property>

50070 - most likely the name node port will be this
so the name node URL will be http://nn01.itversity.com:50070

hdfs-site.xml

<property>
      <name>dfs.namenode.http-address</name>
      <value>172.16.1.101:50070</value>
    </property>


50070 - name node UI runs on this port


du -sh //command to know the size of folder
hadoop fs -du -s -h

Also,
ls -h // command to know the size 
hadoop fs -ls -h

//to get the details of hdfs locations

hdfs fsck /user/maruthi_rao2000/crime -files -blocks -location


yarn-site.xml

    <property>
      <name>yarn.resourcemanager.address</name>
      <value>rm01.itversity.com:8050</value>
    </property>

    <property>
      <name>yarn.resourcemanager.webapp.address</name>
      <value>rm01.itversity.com:19288</value>
    </property>

// If we want to get memory taken by spark jobs, details will be in /etc/spark/conf/spark-env.sh

#SPARK_EXECUTOR_CORES="1" #Number of cores for the workers (Default: 1).
#SPARK_EXECUTOR_MEMORY="1G" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)


// to display number of lines

wc -l file.txt
Total number of files: hadoop fs -ls /path/to/hdfs/* | wc -l
Total number of lines: hadoop fs -cat /path/to/hdfs/* | wc -l
Total number of lines for a given file: hadoop fs -cat /path/to/hdfs/filename | wc -l

// to display the size 

hadoop fs -ls -h <path>

//sqoop connect will work only if relevant jars are present below location
/usr/hdp/current/sqoop-client/lib

--append 
--where "dpt_id >10"

(or)

--incremental append
--check-column dpt_id
--last-value 10


To see compression algorithms supported, check in /etc/hadoop/conf/core-site.xml

<property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
</property>


warehouse-dir creates the folder with table name.
--table orders
--warehouse-dir /user/maruthi_rao2000/sqoop_import/retail_db

If we want to override if the directory already exists in case of using --warehouse-dir, we can use --delete-target-dir

Above command creates orders folder inside retail_db if retail_db exists, otherwise it creates retail_db and create a folder named orders and then copy the records. But target-dir creates files directly under specified folder. But that folder should not exist, unless it is append operation.

If we use target-dir, condition is that the folder should not exist. Then it will create the folder (not the folder with table name) and creates the files inside the folder directly. It will not create folder with table name.

--table orders
--target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders


// If we have to import only those rows from table which has order_id > 50000 we use --boundary-query

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table orders \
  --warehouse-dir /user/maruthi_rao2000/sqoop_import/retail_db \
  --boundary-query "select min(order_id), max(order_id) from orders where order_id > 50000"

// If the table do not have primary key, sqoop import will complain because it cannot perform the default split logic. So it will ask to specify the --split-by column or use -m 1
--split-by column should not have null values.

If you are not sure of the control argument that you need to give when you are splitting by non numeric column then if you execute the command, the error will have the control argument. Use it before --connect

--as-avrodatafile
--as-parquetfile
--as-sequencefile

For compression use

-z or--compress (default gzip)
--compression-codec

To uncompress, copy files to local using copyToLocal and use
gunzip part*.gz


// If we are using custom query like with joins, we cannot give --num-mappers without giving --split-by

// If we use --query we must put 'and \$CONDITIONS' in the query

// If we use --query, we must give --split-by even if there is a primary key

// --table and --columns are mutually exclusive


sqoop export --connect jdbc:mysql://ms.itversity.com:3306/retail_export --username retail_user --password itversity --export-dir hdfs://nn01.itversity.com:8020/apps/hive/warehouse/maruthi_sqoop_import.db/daily_revenue --table daily_revenue_mmsr --input-fields-terminated-by "\001" --m 1


// If we are using hive import with --query you must specify target-dir and --split-by

takeOrdered
takeWhile


def saveAsTextFile(path: String): Unit                                                                      
def saveAsTextFile(path: String, codec: Class[_ <: org.apache.hadoop.io.compress.CompressionCodec]): Unit  

one method stores as a plain text file 
By using another method we can give compression codec of your choice
org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec

sortedRevenue.saveAsTextFile("/user/maruthi_rao2000/dailyRevenueSnappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

If you want to check the file again in scala context, use sc.textFile("/user/maruthi_rao2000/dailyRevenueSnappy")
Note: You don't have to give compression algorithm since snappy is one of the supported forms and mentioned in core-site.xml


To save in different file formats, we need to use dataFrames as RDD doesn't have APIs for fileFormat
DataFrames can be formed using sqlContext and not through sc.
val ordersDF = sqlContext.read.text("/user/maruthi_rao2000/sqoop_import/retail_db/orders")
(or)
val ordersDF = sqlContext.read.json("/public/retail_db_json/orders")

*******************************************************


RDD.saveAsTextFile

DataFrame.save(path, format)
DataFrame.write.<format>(path)

sqlContext.read.<format>(path)
sqlContext.load(format,path)


If we have to save compressed we can to it using rdd

******************************************************

If the file format is text, sc object has API to read it by using sc.textFile
***Using sc.textFile command we can read compressed files also as long as the compression algorithm is supported.
But if the file format is some other industry standard specification like orc, json, parquet, avro etc. we need to use another implicit object called sqlConttext.

val ordersDF = sqlContext.load("/public/retail_db_json/orders","json")
ordersDF.show //to preview
ordersDF.printSchema
ordersDF.select("order_id","order_status").show

****************************************************

Hive language manual

***************************************************

we can add strings or integers with +"\t"+


to unzip gz file use below command
gunzip part-00000.gz

************************************************

If we are giving registerTempTable command there can be error if there is same table name in hive. In that case, change table name


sortByKey() 
sortByKey(false) - descending

To get top N records, we can use take(N) on RDD



=======================================================

sqoop eval  \
--connect jdbc:mysql://ms.itversity.com:3306/retail_db \
--username retail_user \
--password itversity \
--query "select * from orders limit 10"

To import orders table records to hdfs into orders folder, we can do it in two ways
1) 
sqoop import \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --table orders \
 --warehouse-dir /user/maruthi_rao2000/sqoop_import/retail_db

 If we use warehouse-dir it will create folder in the path specified with name as table name.

2) 
If we use target-dir, condition is that the folder should not exist. Then it will create the folder (not the folder with table name) and creates the files inside the folder directly. It will not create folder with table name.

If the folder already exists, it will throw exception saying folder already exist

***** it will not complain if there is --append control argument

sqoop import \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --table orders \
 --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders


 sqoop import - differenet file formats


 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table orders \
  --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_avro \
  --as-avrodatafile 

With compression codec

 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table orders \
  --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_compress \
  --compression-codec org.apache.hadoop.io.compress.GzipCodec

check /etc/hadoop/conf/core-site.xml for all compression algorithms

order-revenue
--------------

select o.order_id, sum(oi.order_item_subtotal) from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_id


sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --query "select o.order_id, sum(oi.order_item_subtotal) from orders o join order_items oi on o.order_id = oi.order_item_order_id and \$CONDITIONS group by o.order_id" \
  --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_revenue \
  --num-mappers 2 \
  --split-by order_id

******************
  If where clause is present in query then give "where \$CONDITIONS and <remaining conditions>". If where clause is not there then give "and \$CONDITIONS before group by". Remember back slash

  With --query, when we use num-mappers, then we should specify split-by
****************

By default sqoop will use query select min(<split-by>), max(<split-by>) from <table name> 

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_boundary \
  --num-mappers 2 \
  --table orders \
  --boundary-query "select min(order_id), max(order_id) from orders where order_id>68840"


  Nulls and delimiters
  ---------------------

  sqoop import \
   --connect jdbc:mysql://ms.itversity.com:3306/hr_db \
   --username hr_user \
   --password itversity \
   --table employees \
   --warehouse-dir /user/maruthi_rao2000/sqoop_import/hr_db/emp \
   --null-non-string -1 \
   --fields-terminated-by "\t" \
   --lines-terminated-by ":"

 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_boundary \
  --num-mappers 2 \
  --table orders \
  --boundary-query "select min(order_id), max(order_id) from orders where order_id>68840"

  append
  -------

  sqoop import \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user \
   --password itversity \
   --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders \
   --table orders \
   --where "order_date like '2013%'" \
   --num-mappers 2

   Appends can be done in following ways

   1) 
   --target-dir
   --query
   --num-mappers
   --split-by
   --append

   2) 

   --target-dir 
   --table orders 
   --where  
   --num-mappers
   --append

   --target-dir 
   --table orders 
   --boundary-query
   --num-mappers
   --append

   3) 

   --check-column
   --incremental append
   --last-value 

 1) sqoop import \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user \
   --password itversity \
   --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_boun \
   --table orders \
   --where "order_date like '2013%'" \
   --num-mappers 2

2) sqoop import \
   --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
   --username retail_user \
   --password itversity \
   --target-dir /user/maruthi_rao2000/sqoop_import/retail_db/orders_boun \
   --table orders \
   --incremental append \
   --check-column order_date \
   --last-value '2013-12-31' \
   --num-mappers 2


hive
----
sqoop import from mysql to hive

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table orders \
  --num-mappers 2 \
  --hive-import \
  --hive-database maruthi_sqoop_import \
  --hive-table orders

**** Check if you gave --hive-import. If not the job will be executed but table will not be created. If we are executing again after correcting, we should delete the temp folder created in root location.


***** In sqoop import, need to check if it is possible to have --query with --where


****** Export table should be present before running sqoop export



***** In sqoop export, if columns order is different in the table compared to the file from which we are trying to export then use columns argument to specify the order of columns to match the order in file.

****** We can give column which is primary key in split-by argument, though we are not grouping by that column in query argument


spark-shell \
 --master yarn \
 --conf spark.ui.port=14623 \
 --num-executors 6 \
 --executor-cores 2 \
 --executor-memory 2G 


**** Above command indicates that we want to have 2 cores in each executor. So, in total we will have 12 cores each taking 2GB of memory.

Absence of record is None, if we do leftOuterJoin (synonymous to null in db)

**** To sort Iterable<V> type of data we need to convert to list by calling .toList on Iterable

list.sorted
list.sortBy(o => -o)

groupByKey() gives (key,Iterable)

tuples.sortByKey()
tuples.sortByKey(false)

Iterable has map method

hive functions
---------------
regexp_replace
substring
nvl
cast(substring() as int)
concat(str1,str2)


val ordersDF = orders.map(order => {
  (order.split(",")(0).toInt, order.split(",")(1), order.split(",")(2).toInt, order.split(",")(3))
  }).toDF("order_id","order_date","order_customer_id","order_status")

round(sum(oi.order_item_subtotal),2)

Hive and spark-shell can be used together
------------------------------------------

hive queries can be executed in spark shell in the following way.

sqlContext.sql("use maruthi_retail_db_orc")
sqlContext.sql("show tables").show


in spark its string.substring(0,2) vs in query it is substr(order_date,0,2)

in spark we can concat strings with '+'

all are functions in sql. Usually we are not using '.'






 



























































